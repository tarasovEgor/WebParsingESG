{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    return webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_links(base_url):\n",
    "    sitemap_index_url = urljoin(base_url, '/sitemap.xml')\n",
    "    all_links = []\n",
    "\n",
    "    try:\n",
    "        res = requests.get(sitemap_index_url, timeout=10)\n",
    "        if res.status_code != 200:\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'xml')\n",
    "        if soup.find('sitemapindex'):\n",
    "            for sitemap in soup.find_all('sitemap'):\n",
    "                loc = sitemap.find('loc')\n",
    "                lastmod = sitemap.find('lastmod')\n",
    "                if loc and lastmod and '2025' in lastmod.text:\n",
    "                    child_sitemap_url = loc.text\n",
    "                    try:\n",
    "                        child_res = requests.get(child_sitemap_url, timeout=10)\n",
    "                        if child_res.status_code == 200:\n",
    "                            child_soup = BeautifulSoup(child_res.content, 'xml')\n",
    "                            for url_tag in child_soup.find_all('url'):\n",
    "                                loc_tag = url_tag.find('loc')\n",
    "                                if loc_tag:\n",
    "                                    all_links.append(loc_tag.text)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[child sitemap error] {child_sitemap_url} – {e}\")\n",
    "        else:\n",
    "            for url_tag in soup.find_all('url'):\n",
    "                loc_tag = url_tag.find('loc')\n",
    "                if loc_tag and '2025' in loc_tag.text:\n",
    "                    all_links.append(loc_tag.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[sitemap error] {sitemap_index_url} – {e}\")\n",
    "\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def get_internal_links(driver, base_url):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if not href.startswith('http'):\n",
    "            href = urljoin(base_url, href)\n",
    "        if base_url in href:\n",
    "            links.add(href)\n",
    "    return list(links)\n",
    "\n",
    "\n",
    "def download_pdfs(driver, base_url, company_inn_folder):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if href.lower().endswith('.pdf'):\n",
    "            pdf_url = href if href.startswith('http') else urljoin(base_url, href)\n",
    "            try:\n",
    "                pdf_name = os.path.basename(urlparse(pdf_url).path)\n",
    "                pdf_path = os.path.join(company_inn_folder, pdf_name)\n",
    "                r = requests.get(pdf_url, timeout=10)\n",
    "                with open(pdf_path, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "            except Exception as e:\n",
    "                print(f\"PDF download error: {e}\")\n",
    "\n",
    "\n",
    "def get_page_text(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "\n",
    "def scrape_company(driver, company, url, inn, output_dir):\n",
    "    company_data = []\n",
    "    parsed_links = set()\n",
    "    company_inn_folder = os.path.join(output_dir, f\"{company}_{inn}\")\n",
    "    os.makedirs(company_inn_folder, exist_ok=True)\n",
    "\n",
    "    sitemap_links = get_sitemap_links(url)\n",
    "    if sitemap_links:\n",
    "        links = sitemap_links\n",
    "    else:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        links = get_internal_links(driver, url)\n",
    "        links.insert(0, url)\n",
    "\n",
    "    for link in links:\n",
    "        if link in parsed_links:\n",
    "            continue\n",
    "        parsed_links.add(link)\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            time.sleep(2)\n",
    "            text = get_page_text(driver)\n",
    "            download_pdfs(driver, link, company_inn_folder)\n",
    "            company_data.append({\n",
    "                'company': company,\n",
    "                'inn': inn,\n",
    "                'url': link,\n",
    "                'text': text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error visiting {link}: {e}\")\n",
    "    \n",
    "    return company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Группа «Илим» (https://www.ilimgroup.ru/ustoychivoe-razvitie/)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    driver = init_driver()\n",
    "    df = pd.read_excel('templates/input_links.xlsx', sheet_name='Лист1')\n",
    "    output_data = []\n",
    "    output_dir = 'downloaded_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        company, url, inn = row['company'], row['url'], row['INN']\n",
    "        print(f\"Scraping {company} ({url})\")\n",
    "        data = scrape_company(driver, company, url, inn, output_dir)\n",
    "        output_data.extend(data)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    pd.DataFrame(output_data).to_csv('scraped_output.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
